---
title: "CBowersoxAssign7"
author: "Cheryl Bowersox"
date: "Saturday, March 14, 2015"
output: html_document
---
# Week 7 Assignment

Cheryl Bowersox 
IS 605 - 1
3/15/15

## Problem Set 1 - 

### Compute expected value and standard deviation

```{r}

library(data.table)
eval <- function(x){
  #x is a numeric vector
  l <- length(x)
  dtx <- data.table(x)[, .N, keyby = x]
  dtx[,p  := N/l]
  e <- sum(dtx[,x] * dtx[,p])
  return(e)
  
}

sval <- function(x){
  s <- sum((x - eval(x))^2)
  s <- sqrt(s/(length(x)-1))
  return(s)
}

# compare eval vs. mean , sval vs sd function 3 times
x <- sample(seq(1:100), 100, TRUE)
eval(x) == mean(x)
sval(x) == sd(x)

x <- sample(seq(1:100), 10000, TRUE)
eval(x) == mean(x)
sval(x) == sd(x)

x <- sample(seq(1:100), 1000000, TRUE)
eval(x) == mean(x)
sval(x) == sd(x)
round(sval(x), 10) == round(sd(x), 10)
```
For some very large x, the sd(x) function returns a slightly different value than the sval(x) function, due to the very large values generated by sval(x), but the the two functions return equal results to at least the 10th place.


### calculating expected value and standard deviation for a ongoing values

Assuming the function will be called for each new value x
```{r}
# assuming independent and identically dist. x, mean(x) is normally dist.
# Using grouped mean to approximate expected value as total x processed > 10k
# 


estval <- function(x){
  l <- length(x)
  ct <<- ct + l  #total number
  
  if(ct < 10000) {  # calculate actual mean/sd if current total processed is less than 10K
    xall <<- c(xall, x)
    e <- eval(xall)
    s <- sval(xall)
  }else{
    # count of xall is greater than 10000
    # create intervals to start grouping
    if(f == 1){
   
      xall <<- c(xall, x)
      mn <- min(xall)
      mx <- max(xall)
      mi <- (mx - mn)/2
      rg <- (mx - mn)
      # how many divde rg into to start
      inv <- seq(mn, mx, round(log(rg)+10)) # shrink into intervals - log of range + 10
      spr <- inv[2] - inv[1]
      #set up first group data table
      dtx <- data.table(sort(xall))
      dtx[, interval := findInterval(V1,inv)]
      dtx <- dtx[, .N, keyby = interval]
      dtx[, lbnd := inv[dtx[,interval]]]
      dtx[, ubnd := inv[dtx[,interval]]+spr]
    
      #calc midpoint
      dtx <<- dtx[, mid := (ubnd + lbnd)/2]
      f <<- 0  #set f = 0 so do not recreate table
      
    }else{
      #process new x's if not first time count > 10k
      
      # need to create new interval?
      if(max(x) > max(dtx[,ubnd])){
        rw <- list(max(dtx[,interval]) +1, 0,
                         max(dtx[,ubnd]),  max(x)*1.01, 
                         (max(dtx[,ubnd]) + max(x)*1.01)/2)
        dtx <- rbind(dtx,rw)
      }
      if(min(x) < min(dtx[,lbnd])){
        rw <- list(min(dtx[,interval]) -1, 0,min(x),
                         min(dtx[,lbnd]), 
                         (min(dtx[,lbnd]) + min(x))/2)
        dtx <- rbind(dtx,rw)
        dtx <- dtx[,interval := interval + 1]
      }
      
      # add new x's to grouped table
      dtx <- dtx[order(interval)]
      dtf <- data.table(sort(x))
      dtf[, interval := findInterval(x,sort(dtx[,lbnd]))]  
      dtf <- dtf[, .N, keyby = interval]
      setkey(dtf,interval)
      dtx <- merge(dtx, dtf, by = "interval", all = TRUE)
      dtx[ , N := sum(N.x, N.y, na.rm=TRUE), by=interval]
      dtx[,N.x := NULL]
      dtx[,N.y := NULL]
      setcolorder(dtx,c("interval", "N", "lbnd", "ubnd","mid"))
      
      
      dtx <<- dtx
    }
    # calc e(x) and sd(x)
    e <- sum(dtx[,mid]*dtx[,N])/sum(dtx[,N])
    s <- sum((x - eval(x))^2)
    s <- sqrt(s/(length(x)-1))
  }  #end of else
  ret <- c(e,s)
  return(ret)
}

#Testing
# set intial 
ct <<- 0
xall <<- c()
f <<- 1
dtx <<- data.table()
y <-c()

# Test function, holding all values in y to compare results against base funcitons
# doing this for very large 
# number of obs. would defeat the pupose of the estimation & grouping

x <- sample(seq(1,700), 3000, TRUE) 
y <- c(x,y)
ret <- estval(x)
(e <- ret[1])  # under 5k returns actual mean
(s <- ret[2])
(e - mean(y))
(s - sd(y))


x <- sample(seq(-15,760), 3000, TRUE)
y <- c(x,y)
ret <- estval(x)
(e <- ret[1])  # under 5k returns actual mean
(s <- ret[2])
(e - mean(y))
(s - sd(y))


x <- sample(seq(-30,870), 3000, TRUE)
y <- c(x,y)
ret <- estval(x)
(e <- ret[1])  # under 5k returns actual mean
(s <- ret[2])
(e - mean(y))
(s - sd(y))

x <- sample(seq(-30,920), 30000, TRUE)
y <- c(x,y)
ret <- estval(x)
(e <- ret[1])  # under 5k returns actual mean
(s <- ret[2])
(e - mean(y))
(s - sd(y))


x <- sample(seq(-30,1000), 30000, TRUE)
y <- c(x,y)
ret <- estval(x)
(e <- ret[1])  # under 5k returns actual mean
(s <- ret[2])
(e - mean(y))
(s - sd(y))

```
For this sample, estimates for E(x) have very small variance to the calculated mean, estimates for SD show a larger variability.


##Problem Set Two

### Calculate correlation and covariance matrices for auto-mpg data 

Co-variance = E[(x-E(x))(y-E(y))]

```{r}
#import data
# Read matrix data from data on github allows others to access data
library(RCurl)
options(RCurlOptions = list(cainfo = system.file("CurlSSL", "cacert.pem", package = "RCurl")))
fileurl= "https://raw.githubusercontent.com/cherylb/CompMath/master/auto-mpg.data"
data <- getURL(fileurl)
df <- read.table(text = data)

names(df) <- c("d", "h", "w", "a", "m")

n <- ncol(df)
k <- nrow(df)
Co <- diag(0,n)
A <- as.matrix(df[1:5])
n <- nrow(df)
m <- ncol(df)

N <- matrix(1,nrow=n)

B <- A - N %*% t(N) %*% A / n 
Q <- t(B) %*% B / (n-1)

# sample covariance
(Q)
#test matches rounded to 9 places
print("testing: matches base function to at least places")
(round(Q - cov(df),9))

#Coorelation matrix
C <- matrix(,nrow = m, ncol=m)

j <- 1
while(j <= m){
  i <- 1
  while(i <= m){
    C[i,j] <-   Q[i,j]/(sqrt(Q[i,i])*sqrt(Q[j,j]))
    i <- i +1
  }
  j <-  j+1
}

# sample covariance
# Correlation:
(C)
#test matches rounded to 9 places
(round(C - cor(df),9))




```