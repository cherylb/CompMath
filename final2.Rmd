---
title: "Final"
author: "Cheryl Bowersox"
date: "Friday, May 22, 2015"
output: html_document
---

## Review of Essential Concepts

```{r}
library(pracma)

# function for converting matrix format from R
convmat <- function(X){
  M <- paste("\\begin{bmatrix}","\n")
  
  i = 1
  n = nrow(X)
  if(is.null(n)){
    n <- length(X)
    while(i <= n){
       M <- paste(M, paste(round(X[i],4), collapse = " & "),"\\\\ \n")
       i = i + 1
    }
  }else{
    while(i <= n){
       M <- paste(M, paste(round(X[i,],4), collapse = " & "),"\\\\ \n")
       i = i + 1
    }
  }
  M <- paste(M, "\\end{bmatrix}","\n", collapse = "")
  return (M)
}

mag <- function(x){
  return(sqrt(sum(x*x)))
}

proj <- function(v, u){
  #projection of v onto u
 return(sum(v*u)/mag(v)^2 * v)
}
```

```{r}
A <- matrix(c(1,2,-2,1,-1,1,3,5,-2,5,-9,4), nrow = 3)
str <- convmat(A)
```

$$
  A = 
  `r convmat(A)`\\
$$

**1. Rank of matrix A**
  The rank of matrix A is equal to the number of non-zero rows in row echelon form

$$
  `r convmat(rref(A))`\\
$$
has 3 non-zero rows and has a rank of 3

**2. Transpose of A**
  The Q = transpose of A:
  
$$
  Q = 
  `r convmat(t(A))`\\
$$

**3. Orthonormal basis vectors**
normalized vi * vi  = 1
orthogonal to each other  vi * vj = 0
```{r}
# proj(u1) onto v2  = u1 dot v2 over mag(v2) squared times v2
v1 <- A[,1]
v2 <- A[,2]
v3 <- A[,3]
v4 <- A[,4]

u1 <- v1
u2 <- v2 - proj(u1,v2)
u3 <- v3 - proj(u1,v3) - proj(u2,v3)
u4 <- v4 - proj(u1,v4) - proj(u2,v4) - proj(u3,v4)

e1 <- u1/mag(u1)
e2 <- u2/mag(u2)
e3 <- u3/mag(u3)
e4 <- u4/mag(u4)
B <- as.matrix(cbind(e1,e2,e3,e4))
```

$$
  B = 
  `r convmat(B)`\\
$$

  are an orthonormal basis for matrix A\\


**4. Characteristic polynomial of A**
```{r}
A <- matrix(c(4,1,-1,0,-2,0,5,3,6), nrow = 3)
M <- rref(A)
```
$$
  A = 
  `r convmat(A)`\\
$$

$$
  B =
  \lambda `r convmat(diag(3))` A\\
  Det(B) = -\lambda^3 + 8\lambda^2 -9\lambda - 58\\
$$

**eigenvectors & values of A  **
```{r}
vals <- eigen(A)$values
vecs <- eigen(A)$vectors
```
Eigenvectors of A are
$$
   E1 = 
   `r convmat(vecs)`\\
$$

Eigenvalues of A are
$$
  E2 = 
  `r vals`\\
$$


**6 high bias and high variance**

High bias means that the model itself does not fit the true underlying function very well, the actual points in the given data may be close to values predicted by the model (low variance) but the model is different from the true shape of the data. A good example of this is a higher order polynomial that reduces the variance by accounting for the fluctuations in the data, but does not actually align well to the true nature of the function.
A model with high variance may more closely follow the underlying function, but data points within the selected sample are not very close to the model results.

**7 mean value of repeated samples**

The mean value of repeated samples from a uniform distribution will have a normal distribution

**8 Derivativef(x) = $e^xcos^2(x)$**  
h(x) = $e^x$  
g(x) = $cos^2(x)$  
h'(x)= $e^x$  
g'(x) = $-2cos(x)sin(x) = -sin(2x)$  
f(x) = h'(x)g(x) + h(x)g'(x)  
f(x) = $e^xcos^2(x) - e^xsin(2x)$  
f(x) = $e^x(cos^2(x) - sin(2x))$  

**9 Derivative f(x) = $e^{sin(x^{2})}$**  
g(x) = $e^x$  
g'(x) = $e^x$  
h(x) = $sin(x)$  
h'(x) = $cos(x)$  
m(x) = $x^2$  
m'(x) = $2x$  
f'(x) = $e^{sin(x^{2})}cos(x^2)2x$  

**10 integrate F = $\int e^xcos(x)\,dx$**  

u = $e^x$
du = $e^x$
dv = $cos(x)$
v = $sin(x)$


F = $\int u dv\,dx = u v - \int v du\,dx$  
F = $\int(e^x)cos(x)\,dx = e^xsin(x) - \int(e^x)sin(x)\,dx$  
F = $e^xsin(x) - e^xcos(x) + \int(e^x)cos(x)\,dx$  
$2\int(e^x)cos(x)\,dx  = e^xsin(x)  - e^xcos(x)$  
F = $1/2e^x(sin(x) + cos(x))$    


## mini assignments

** 1 Bayes rule **
G = good score
B = bad score
D = default rate

*assuming scores can either be good or bad*
P(B) = .3
P(G) = .7
P(D|G) = .04
P(D|B) = .20
P(D) = P(B)P(D|B) + P(G)P(D|G) = .088

P(B|D) =  $\frac{P(D|B)P(B)}{P(D)}$
P(B|D) = $\frac{.2 * .3}{.088}$
P(B|D) = 'r .06/.088'


```{r}
library(gRain)

score <- c("good", "bad") # credit can be good or bad
fault <- c("no", "yes") # loan default no / yes
c <- cptable(~credit, values = c(.7,.3), levels = score) # score, good / bad
d <- cptable(~default|credit, values = c(.96,.04,.80,.20), levels = fault) #defaut no/yes given credit good /bad 
plist <- compileCPT(list(c,d))
plist$default

net1 <-grain(plist)
```

*looking at probablity of bad credit overall and given a default*
```{r}
querygrain(net1, nodes=c("credit"), type="marginal")

#Evidence that loan is default
net12 <- setEvidence(net1, nslist=list(default = "yes"))
#New prob. difficulty 
querygrain(net12, nodes=c("credit"), type="marginal")
````
Probability of bad credit increase from 30% to about 68% given a default, this result aligns with the previously calculated amount.


**2.  Central Limit Therom**

* Creating a random sample of log-normal distribution with mean 40 and sd 10:
```{r}
samp <- function(n){
  x <- rlnorm(n,meanlog=40,sdlog = 10)
  return(x)
}

#  single sample of size 100 gives
sample.mean <- mean(log(samp(100)))
sample.sd <- sd(log(samp(100)))
```
Both the sample mean and sample standard deviation are close to the theoretical mean: 
The mean of sample size 100 is `r sample.mean`
The standard deviation of the sample size 100 is `r sample.sd`

* Sample of 100 drawn from population with mean 50 and sd 25. Because of the large sample size the sample mean will be close to 50, the sample sd will be close to 25. The larger the sample size the closer they will become. 

* Probablity that mean of sample is > 45
The sample mean is normally distributed 
$\mu_{\bar{x}} = 40$ and $\sigma_{bar{x}} = \frac{25}{\sqrt{100}= 2.5$
P(x > 45) = 0.0228


**3 sampling from a function

```{r}
pofx <-  function(x){
  m <- 10
  p <- .3
  return(choose(m, x) *p^x* (1-p)^(m-x))
}
  
  
samp.b <- function(n){
  # n is size of each sample
  s <- seq(0,10)
  vals <- sample(s,n,replace = TRUE)#values in range 0 to 10
  p <- sapply(s,pofx)  #prob of each value generated
  ret<- sample(s, 1000, replace = TRUE, prob = p) # get a return sample
  return(ret)
}
```

*Density for 1000 indivdual samples
```{r}
library(ggplot2)
n <- 1000
df <- data.frame(x = samp.b(n))
ggplot(data = df, aes(x=x)) +
    geom_histogram(aes(y = ..density..), binwidth = 1,
                   fill = "darkcyan", alpha = 0.4)

```

**2.4 Principle component analysis**

```{r}
#load cars data

library(RCurl)
options(RCurlOptions = list(cainfo = system.file("CurlSSL", "cacert.pem", package = "RCurl")))
fileurl= "https://raw.githubusercontent.com/cherylb/CompMath/master/auto-mpg.data"
data <- getURL(fileurl)
df <- read.table(text = data)

names(df) <- c("displace", "horsepwr", "weight", "acceler", "mpg")

pairs(df)

normx <- function(x){
  # x is a vector
  m <- (x - mean(x))/sd(x)
  return(m)
}

df <- sapply(df,normx)
A <- cov(df[,1:4])

#SVD of A
D <- svd(A)$d
V <- svd(A)$v
# calc scores
scores <- df[,1:4]%*%V
s <- sqrt(D) #scale by sd
#two vest vectors

plot(scores[,1],scores[,2],
     xlab= 'displace',ylab='horsepwr')


# First plot as vectors
arrows(x0=0,y0=0,x1 =V[1,1]*3.5,y1= V[2,1]*3.5,length=0.1, lwd=2,angle=20, col="blue")
arrows(x0=0,y0=0,x1 =V[1,2]*3.5,y1= V[2,2]*3.5,length=0.1, lwd=2,angle=20, col="green")
```

**2.5 Sampling in bootstapping**  
When sampling from the total poulation with replacement, a single value x has a probablity of 1/n chance of being selected, and a 1-1/n chance of not being selected.  
For n selections, any particular value of x has a (1 -1/n)^n chance of not being selected.  
As the sample size n increases the probablity of not being selected becomes
$\lim_{x\to\infty}(1-1/n)^n = e^-1$  
$e^-1 \approx .368$  
The probablity of selection is $1-e^-1 \approx \.632$  

## mini project

```{r}
# import data
# local for reasons
x <- read.table("CompMath/ex3x.dat")
names(x) = c("area", "beds")
y <- read.table("CompMath/ex3y.dat")
names(y) = "price"
dfxy <- data.frame(cbind(x,y))
means <- sapply(dfxy,mean) # in case we want it back later
sds <- sapply(dfxy,sd)

# standard
dfxy <- sapply(dfxy,normx)

z <- rep(1,nrow(dfxy))
dfxy <- cbind(z,dfxy)
X <- as.matrix(dfxy[,1:3])
Y <- as.matrix(dfxy[,4])

findtheta <- function(X,Y,alpha, n){
  # X is X value
  #Y is Y values
  # alpha is increment
  # n is itterations
  cost_hist <- vector()
  theta_hist <- list()
  
  theta <- matrix(c(0,0,0),nrow =3) # start at zero

  errcost <- function(X,Y,theta){
    #X is matrix of x values
    #y is outcome
    #theta is level
    c <- sum((X %*% theta - Y)^2)/(2*length(Y))
    return(c)
    }
  
  i <- 1
  while(i <= n) {
    err <- as.matrix(X %*% theta - Y)
    delta <- t(X) %*% err / nrow(Y)
    theta <- theta - alpha * delta
    cost_hist[i] <- errcost(X, y, theta)
    theta_hist[[i]]<- theta
    i <- i +1
  }
  ret <-  list("t" = theta, "ch" = cost_hist, "th" = theta_hist)
  return(ret)
}
```


**model for alpha =1, n = 50
```{r}
alpha = .01
n = 50
result <- findtheta(X,Y,alpha,n)
#model
b <- result$t[1]
v1 <- result$t[2]
v2 <- result$t[3]

yhat1 <- b + v1*X[,2] + v2*X[,3]

#plot error as funciton of itterations
plot(1:n,result$ch, main = "Alpha = .01, n = 50", xlab = "itterations",
     ylab = "cost")

```

**model for alpha =.001, n = 50
```{r}
alpha = .001
n = 50
result <- findtheta(X,Y,alpha,n)
#model
b <- result$t[1]
v1 <- result$t[2]
v2 <- result$t[3]

yhat1 <- b + v1*X[,2] + v2*X[,3]

#plot error as funciton of itterations
plot(1:n,result$ch, main = "Alpha = .001, n = 50", xlab = "itterations",
     ylab = "cost")

```

**model for alpha =.0001, n = 50
```{r}
alpha = .001
n = 50
result <- findtheta(X,Y,alpha,n)
#model
b <- result$t[1]
v1 <- result$t[2]
v2 <- result$t[3]

yhat1 <- b + v1*X[,2] + v2*X[,3]

#plot error as funciton of itterations
plot(1:n,result$ch, main = "Alpha = .0001, n = 50", xlab = "itterations",
     ylab = "cost")
```

* compared to lm model
```{r}
yhatlm <- lm(Y ~ (X[,2]+ X[,3]))
yhat2 <- fitted(yhatlm)
plot(1:length(yhat2),yhat2-Y, main = "R model error")
abline(0,0)
plot(1:length(yhat1),yhat1-Y, main = "Gradient Descent model error")
abline(0,0)
plot(1:length(yhat1),yhat1-yhat2, main = "Diff: R model to Gradient")
abline(0,0)
```

*5 fold cross-validation
```{r}
library(boot)

#for R model
df <- data.frame(cbind(X[,2:3],Y))
cst <- cbind(Y,yhat2)
fit2 <- glm(data = df, V3~(V1 + V2))

cv.error <- cv.glm(df,fit2, K = 5)$delta
